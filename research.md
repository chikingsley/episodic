Duolingo’s Engineering & Product Evolution – A Comprehensive Analysis

1. Mobile Architecture & UI

Duolingo’s mobile app architecture has evolved from a monolithic client state to a more modular, server-driven model focused on rapid iteration and cross-platform consistency. In the early days, Duolingo’s app (especially Android) used a single centralized state (inspired by Redux/Elm) for UI, but this proved hard to scale as the app grew in complexity. Frequent state updates could trigger unnecessary re-renders and slow performance. In 2021, Duolingo “rebooted” its Android architecture by adopting the Repository + MVVM pattern – breaking the monolith into multiple state stores (“repositories”) and introducing ViewModels to derive view-specific state. This aligned with Google’s recommended practices and immediately improved frame rates and visual consistency. The lesson for a new product is clear: start with a modular architecture (e.g. MVVM or equivalent) to avoid painting yourself into a corner with a global state pattern that lacks platform support.

Today, Duolingo pushes the envelope with Server-Driven UI (SDUI). Instead of hard-coding all UI flows on the client, the app fetches UI configurations from the server, allowing Duolingo to update interfaces and run UI experiments without requiring a new app release. For example, Duolingo’s Shop screen can be restructured via a server response – the team tested a scrollable carousel vs. a grid for item display entirely through SDUI, avoiding the typical weeks-long app store update cycle. All users, regardless of app version, see the change simultaneously, and bugs can be fixed by adjusting the server payload rather than shipping a new binary. Embedded Image: Below, the left is the control UI and the right is an experimental UI for the Shop, delivered via server config (note the different gem counts and item layouts). This ability to A/B test and tweak UI server-side has greatly accelerated Duolingo’s development cadence.

Duolingo’s Shop screen – Control vs Experiment – delivered via Server-Driven UI. The backend can rearrange UI elements (e.g. item order, currency display) without forcing a client update, enabling rapid experimentation.

On the cross-platform front, Duolingo historically maintained separate native apps for iOS and Android, prioritizing performance on each platform. The Android reboot effort in 2021 unified patterns with iOS to some extent (e.g. embracing native architecture conventions), but Duolingo did not fully adopt a cross-platform toolkit like React Native for the core app. (Community sources indicate Duolingo uses Unity for certain animated features and tried React Native for some projects, but the main app remains largely native for optimal performance.) In recent years, Duolingo also invested in mobile dev experience: they sped up Android/iOS build times by 68%, from ~50 minutes to ~16 minutes, by upgrading CI machines (using AWS r7a instances for Android and M2 Mac minis for iOS) and optimizing build pipelines. They parallelized tasks, introduced build caching (Gradle remote cache, SwiftPM caching), and refactored slow build steps (replacing Kotlin annotation processors with KSP). The result is faster iteration for engineers and the ability to ship updates weekly. For a new 2025 product, the takeaways are: design your client architecture with scalability in mind (e.g. use MVVM or component-based UI from the start), consider server-driven UI or code-push mechanisms to decouple deployment from app store releases, and invest early in CI/CD tooling to keep your developers productive. Duolingo’s journey shows that paying down tech debt (like refactoring a mis-scaled architecture or trimming a bloated app) is possible but costly – it’s better to avoid the debt in the first place if you can. A modern team could also consider using a cross-platform framework (React Native or Flutter) if it can meet performance needs, to accelerate development; Duolingo’s choice to stay mostly native was driven by scale and legacy, but a smaller new product might move faster with shared code.

2. Backend Infrastructure

Duolingo runs on a cloud-native stack that has shifted from a single monolithic backend to a microservices-oriented architecture on AWS. Early on, Duolingo’s backend was a monolith (Python-based) serving all functionalities. As the user base exploded, they migrated to a Docker-based microservice architecture, breaking the system into independent services (as noted by AWS case studies) to improve scalability and team autonomy. Today Duolingo leverages Amazon AWS extensively, using managed databases and services: transactional data lives in Amazon RDS (a mix of MySQL/Postgres), and Amazon DynamoDB is used heavily for high-scale, low-latency storage ￼. In fact, Duolingo stores on the order of 31 billion items in DynamoDB to support its learning sessions and user progress at massive scale ￼. They choose storage based on access patterns: for example, the personalized practice system (“Birdbrain”) writes each learner’s proficiency scores into a single DynamoDB row per user-per-course to get constant-time updates and lookups. This design minimized API latency for updates, though it led to a tradeoff: running multiple AI models in parallel doubled the write frequency and cost – a challenge they solved by buffering writes in memory and batching updates (more on that in section 4). Duolingo’s experience suggests that a new product should design data schemas with scale in mind – e.g. prefer append-only or batched updates over hot overwrites when possible, and use the right database for the job (NoSQL vs SQL) to balance consistency vs performance.

In terms of compute, Duolingo’s services are primarily written in Python, taking advantage of the team’s familiarity and Python’s rich ecosystem for things like AI. However, synchronous Python web services can become a throughput bottleneck. Rather than rewritting everything in Go or Java, Duolingo undertook an async Python migration in 2024-2025 – allowing Python services to handle more I/O-bound concurrency using asyncio. They found that an async version of a service could handle ~40% more requests per instance than the sync version, significantly improving throughput and resource efficiency. This migration was non-trivial: it meant reworking HTTP clients, auth libraries, and re-thinking how they structured code to avoid blocking calls. The payoff was considered worth it as a middle path – it gave a big performance boost without the full cost (and risk) of rewriting services in a lower-level language like Go. For new builders, the lesson is to choose your backend stack wisely: if you anticipate very high throughput, starting with an async-first runtime (Node.js, Go, etc.) or a compiled language can save headaches. Duolingo’s approach shows you can scale with Python successfully, but you’ll eventually need to invest in optimizations or concurrency patterns as you grow. A fresh 2025 project could avoid some legacy baggage by using modern frameworks (for instance, using Python FastAPI + async from day one, or opting for a Go service for critical paths).

On the cloud architecture & costs side, Duolingo emphasizes observability and efficiency. They use tools like CloudZero to break down AWS spending by service and find anomalies ￼. This led to discoveries like a staging environment inadvertently left scaled-up (burning money) and old resources (ElastiCache clusters, unused databases) from legacy features that were still incurring cost ￼ ￼. Once identified, they cleaned these up. They also enabled DynamoDB Time-to-Live (TTL) on tables to auto-expire old data (something they hadn’t done initially, resulting in tables bloated with stale data) ￼. Another focus was rightsizing compute: Duolingo found many services were over-provisioned for peak load. By tuning auto-scaling and lowering default instance sizes, they increased average CPU/memory utilization from single-digit percentages to a healthy ~60%+ range, saving substantially on cloud bills ￼ ￼. Embedded Image: The graph below (from Duolingo’s internal dashboard) illustrates a before-and-after of one service’s CPU usage. In the “overscaled” case (top), CPU usage hovers around 1–3% per instance – a red ❌ indicating waste. After optimization (bottom), the service runs closer to 40–50% CPU on fewer instances – a much more efficient use of resources (starry-eyed emoji indicating success) ￼. Duolingo reports one service’s tweak saved “hundreds of thousands a year” in cost ￼.

Internal monitoring of CPU for a Duolingo microservice, before (top) vs after (bottom) scaling optimizations. Initially, many instances ran at ~2% utilization (red “X”), wasting resources. After rightsizing and tuning auto-scaling, the service runs at ~40% average load (green lines), indicated by the celebratory emoji ￼. This change cut cloud costs dramatically.

Architecturally, Duolingo’s backend is now a collection of specialized microservices (many in Python, some for data pipelines, etc.) running on AWS (likely on ECS or Kubernetes). They coordinate via APIs and message queues. One trade-off of microservices is complexity in orchestration and monitoring – Duolingo’s investment in metrics, logging, and cost tracking was essential to manage this. They integrate cloud spend data into their metrics system and even track costs for external services like OpenAI (which became significant after adding GPT-4 features) ￼. A new product team should similarly plan for observability from the start: know where your dollars go, and instrument your services for performance and cost. Missed opportunities? Duolingo’s reliance on CPython meant hitting the global interpreter lock (GIL) issues and requiring an async rework; a new team could consider using a GIL-free runtime or a faster language early on for critical components. Also, Duolingo initially didn’t enforce TTLs or log hygiene – accumulating years of logs and data – leading to unnecessary cost. A fresh service should enforce data retention policies and log sampling from day one to avoid a “data swamp” and bill shock. Overall, Duolingo’s backend journey teaches balancing quick delivery (Python, managed services) with strategic refactors and cost discipline as you scale.

3. AI/ML Content Generation & Personalization

Duolingo is fundamentally an AI-driven learning app, and over the years they’ve supercharged content creation and personalization with machine learning – from automating podcast-like lessons to powering conversational features with GPT-4. One flagship example is DuoRadio, an audio lesson feature. Originally, creating DuoRadio episodes was a manual, labor-intensive process: scripting dialogues aligned to the curriculum, hiring voice actors, recording and editing audio, etc.. This didn’t scale – in a year they managed ~300 episodes for just a few courses. To scale DuoRadio to many more languages and millions of learners, Duolingo turned to generative AI. In a 2023 hackathon, a team prototyped using large language models (LLMs) to generate episode scripts automatically. The breakthrough was feeding the model with existing course content and example exercises as a guide, rather than prompting it from scratch. By constraining the AI with Duolingo’s own curriculum examples, they got much higher-quality scripts that needed minimal editing. They then used text-to-speech (TTS) technology to generate the audio, likely leveraging or fine-tuning voices for Duolingo’s characters (since Duolingo has custom TTS voices for its cast – more on that shortly). The result? DuoRadio’s daily active users jumped from 100k to 5.5 million, while content production cost dropped by 99%. This dramatic improvement shows the power of generative AI to 10× content creation when applied with domain constraints. For a new product, the lesson is to leverage AI to scale content without losing quality – often the best results come from combining AI with your proprietary data or expertise (in Duolingo’s case, feeding the model a structured curriculum made the outputs far more usable).

Duolingo also embraced cutting-edge AI in its core product with the introduction of “Duolingo Max” features in 2023, powered by OpenAI’s GPT-4. They built two features: Explain My Answer (an AI tutor that gives learners personalized explanations of their mistakes) and Roleplay (an interactive chatbot conversation with Duolingo’s characters). These features required close collaboration with OpenAI – Duolingo fine-tuned prompts and behavior for months to ensure the AI responses stay on target for language learning. Roleplay in particular is a mini revolution in language learning: learners can have an open-ended back-and-forth dialog (text-based) with an AI persona (like ordering coffee from an AI barista character in French), and the AI will respond in context, then at the end provide feedback on the learner’s use of the language. This is all done on-device in the app, making each conversation unique. It’s a feature that simply wasn’t possible to this quality before GPT-4. The trade-off of using such advanced AI is cost and unpredictability – Duolingo likely incurred significant API costs for OpenAI and had to build guardrails for when the model says something incorrect or inappropriate. They mitigate that by allowing users to report bad AI answers and continuously retraining on those examples. For new products now, it’s much more feasible to include generative AI (thanks to APIs and even open-source models), but one should be prepared to handle moderation, quality control, and cost management for these models. Duolingo’s approach of gradually refining the AI in a specific domain (language learning dialogues) is a best practice to emulate, rather than using AI “magic” without oversight.

Another area where Duolingo marries AI and content is in making their characters come alive with speech and animation. Duolingo has a cast of cartoon characters that appear in lessons and stories. To give each character a distinct voice, Duolingo built custom text-to-speech voices for them. This was a complex ML project: they essentially trained voice models (likely using a mix of voice actor recordings and synthesis techniques) so that any sentence in the course can be spoken in, say, Lily’s voice or Oscar’s voice, with the right emotion and personality. They did this because hearing a variety of voices improves learners’ listening skills ￼, and it adds a fun persona element to the lessons. Once characters had voices, Duolingo took it a step further by tackling the viseme generation problem – i.e., making the characters’ mouth movements sync with the spoken audio. Doing this by hand for thousands of sentences was impossible, so Duolingo devised an automated pipeline: when a character’s TTS audio is generated, Duolingo runs it through in-house speech recognition to get the timing of each phoneme (sound). Each phoneme is mapped to a “viseme,” a predefined mouth shape for the character. They then use a tool called Rive (a real-time animation tool) to programmatically animate the character’s mouth according to those visemes. Embedded Image: Below is a peek at Duolingo’s character animation rig in Rive, showing the state machine logic (bottom) that controls animations and the character “Lily” with adjustable mouth positions (center). This system allows Duolingo’s ten cartoon characters to lip-sync any sentence in 40+ languages automatically. It’s a beautiful example of AI-assisted content creation: the learner sees the character speak and the mouth movements match the audio, all without an animator hand-tuning it for each line.

Duolingo’s animation editor (Rive) for character lip-sync. On the bottom is a State Machine defining animation states and transitions (visemes) for speech. The character “Lily” (center) has mouth shapes that are triggered based on phoneme timings extracted by Duolingo’s speech recognition models. This pipeline lets any generated speech be automatically animated, bringing characters to life at scale.

On the personalization side, Duolingo’s secret sauce is Birdbrain, their in-house AI scheduling system. Birdbrain uses machine learning to model each learner’s proficiency on various vocabulary and grammar skills. Every time you complete an exercise, Birdbrain updates its estimate of what you know and how likely you are to recall it. Initially, Birdbrain was a single model that decided when to give you practice on old material vs. new material. Duolingo’s culture of experimentation (“test everything”) led them to develop Birdbrain v2 and beyond – but to validate any new personalization model, they needed to A/B test it against the old one on live users. This created a huge engineering demand: for every user action, they had to run two versions of Birdbrain (one for each model being tested) and compare outcomes. Essentially, the cost of personalization doubled when experimenting with a new model, both in computation and in data storage (since they’d log data for both models). To handle this, Duolingo optimized aggressively: they sharded their event processing by user and buffered updates in-memory so that multiple events for the same user could be processed together. They also applied an LRU cache to drop infrequently used data from memory if needed. By writing to the database less frequently (amortizing multiple exercises per write) and only keeping the latest state per user, they cut the cost of running two models in parallel by 50% – in fact, after these improvements, running two models concurrently was cheaper than what a single model used to cost! The key architectural decision here was to trade a bit of memory and slight delay for a huge win in throughput and cost. For new products that rely on heavy personalization or analytics, this is insightful: if you design your data pipeline to allow batching (even per user or per session), you can achieve big efficiencies. Also, invest in experiment frameworks that let you toggle models or algorithms for subsets of users. Duolingo’s experiment infrastructure is quite sophisticated – features are rolled out behind flags, and multiple experiments run at any given time to test learning outcomes and engagement. This experimentation culture allowed them to evolve Birdbrain continuously and prove via data that newer models improved retention or learning.

Beyond product ML, Duolingo also uses AI to improve engineering itself. A striking example is how they reduced regression testing effort using a GPT-powered tool (called GPT Driver). Mobile apps often require manual QA to click through flows and ensure nothing broke, which is time-consuming. Duolingo partnered with an AI tool that takes natural language test scripts (e.g. “Go to the profile tab, then open settings”) and the GPT-based agent executes those steps in a simulator ￼ ￼. Initially, the team struggled because Duolingo’s app has many variants (due to A/B tests – the next screen isn’t always predictable) ￼. They discovered a prompt engineering trick: instead of scripting every step explicitly, they give GPT Driver a goal like “Complete a lesson and reach the lesson complete screen,” and let the AI figure out the steps on each variant of the app ￼. This made tests more robust to changes ￼. The outcome is that a large portion of their weekly regression tests can be done by the AI agent, with QA engineers simply reviewing the recorded test runs (videos) for any visual anomalies. They reported this cut manual regression testing by about 70% ￼! The drawback is that sometimes the AI might cleverly navigate around a bug (so you could miss an issue), but having humans review the recordings mitigates that ￼. For a new team, leveraging AI for QA or code quality is an area of “easy win” – today’s tools can generate unit tests, help write code (Copilot), and even drive UIs as Duolingo did. Embracing these can free up developer time to focus on product improvements rather than rote testing.

In summary, Duolingo’s AI/ML journey shows a pattern of human expertise + AI leverage: they feed AIs with curated data (curriculum, goals) to get useful content, they combine AI with humans-in-the-loop (e.g. reporting GPT-4 mistakes, reviewing test runs) to ensure quality, and they continuously iterate their models through experimentation. Missed opportunities or things to do differently? It’s hard to criticize Duolingo here, as they’ve been quite forward-thinking. Perhaps an area a new product could improve is using on-device ML more (Duolingo’s personalization mostly happens server-side; a new app might do federated learning or on-device adaptation for privacy and efficiency). Also, Duolingo’s initial approach to generating content via AI (fully from scratch) didn’t work well – the lesson for others is to integrate AI gradually and use your own data to ground it. Now that generative models are more accessible, a startup in 2025 might build AI-driven personalization from day one (whereas Duolingo had to invent Birdbrain over years) – but one should still measure impact via A/B tests, just as Duolingo does, to ensure the AI is actually helping learners.

4. Data Pipelines & Experimentation

Duolingo is obsessed with A/B testing and data, and they’ve built robust data pipelines to fuel that experimentation culture. Every new feature or change at Duolingo is typically rolled out via controlled experiments – this requires an infrastructure to log user events, analyze metrics, and ensure experiments are comparable. To manage the firehose of data (tens of millions of daily users, each generating events), Duolingo created a Data Refinery team that treats “data as code.” They articulated a great principle: “Modeling data isn’t so different from designing an API” ￼. In practice, this means they have a structured process to take raw event logs (e.g. every exercise result, every session) and transform them into higher-level tables that analysts and algorithms can use. They apply software engineering practices to this data ETL: version control, code reviews, linting, tests, and CI/CD. For example, they wrote custom SQL linters to enforce naming conventions and schema consistency – if a developer creates a table column userId in one place and user_id in another, the linter flags it in CI to enforce a single style ￼ ￼. This prevents a proliferation of incoherent datasets and makes it easier for anyone in the company to navigate the data. They also built a “data diff” tool that automates comparisons of new vs. old versions of a dataset ￼. When a data engineer makes a change to how a metric is computed, the system can generate a dev version of the table and do a diff against the production version, commenting the results on the pull request. This encourages developers to verify that their change only affects the intended records. It’s essentially a test harness for data transformations – an excellent idea to catch issues before they affect analysis.

Another challenge in data pipelines is deploying changes without disrupting downstream consumers. Duolingo uses a blue-green deployment approach for data: when they need to recompute a dataset (say they fixed a bug in computing “weekly active users”), they will backfill the entire dataset into a new table, then atomically swap it as the new production table ￼. This way, there isn’t a period where partial data could confuse analysts or A/B metrics – anyone querying sees either the old stable data or the fully new data, never a mix ￼ ￼. They mention this prevents situations where a graph shows a sudden spike just because data was mid-update ￼. For a modern data-driven app, these are great practices: enforce dataset contracts and make changes atomic.

On the experimentation side, Duolingo’s framework ensures that any new feature can be ramped up via percentages or user segments, and crucially, that results are measurable. Their systems measure retention, engagement, and learning outcomes for experiment vs control groups. An interesting internal tool Duolingo built is for the Duolingo English Test (DET) – an online exam they offer. They needed to ensure bias-free proctoring, so they combined human proctors with AI vision models to watch test-takers. They use object detection to flag if you’re wearing headphones (not allowed) or if someone else appears on screen, and gaze detection to see if you keep looking off-screen (possibly at notes). By doing this, they reduced human error and bias in test monitoring. It’s a case of using AI to enforce exam rules at scale, which is analogous to experimentation in that it requires careful validation to not false-flag honest users. While not directly about A/B testing, it underscores Duolingo’s knack for automating trust and safety through data. A new product with user-generated content or online exams could take a page from this by integrating anomaly detection from day one.

One area Duolingo learned from experience was data cost management. Logging everything at fine granularity (every tap, every screen) can lead to massive CloudWatch or S3 costs. They discovered they were keeping extremely verbose logs (including full stack traces) in production ￼. Engineers weren’t initially aware how that adds up. Once they audited it, they dialed back unnecessary logging and implemented sampling. The result was huge savings on CloudWatch bills with no loss of insight ￼. The takeaway: not every event needs to be logged, and certainly not at DEBUG level in prod. For experiments, it’s more important to log key metrics cleanly than floods of data that won’t be used. A nimble 2025 startup should define early what metrics matter (e.g. daily active users, conversion funnels, etc.), instrument those events with analytics, and possibly skip or sample less impactful data (with the option to turn on detailed logging temporarily when needed).

Duolingo’s experiment philosophy is famously rigorous. As they put it, “At Duolingo, we test everything.” ￼. This even applied to infrastructure changes: when they revamped the Android app architecture, they couldn’t A/B test it in the usual way (you can’t have two versions of the entire app running simultaneously for users), so they did a gradual rollout by app version and carefully watched metrics like crash rate, session time, etc., comparing before vs after ￼. For features that they can split by user, they often start with employees (dogfooding), then a small % of real users, then ramp up. For example, the Friend Streak feature (shared streaks with friends) went through an internal prototype and dogfooding phase before public launch. During design, they anticipated future experiment needs: they limited the initial launch to up to 5 friends streaks not just for UX simplicity, but because they wanted to test later whether allowing 6, 7, or unlimited friends yields better retention. So they built the UI and backend to handle a variable number of friend streaks from the start, making it easy to experiment on that parameter without a redesign. As a result, when data showed diminishing returns beyond 3-5 friends, they didn’t bother expanding further unnecessarily. This is a brilliant example of designing for experimentation. A new product team should ask, for each new feature: “how will we know if this works?” and instrument it accordingly. If your feature might later extend (more friends, more content, different timing), consider building in that flexibility so you can A/B test it. It’s a form of future-proofing that paid off for Duolingo.

Missed opportunities or improvements: Early on, Duolingo likely didn’t have such a clean data pipeline – the initiative to treat data like code came as they scaled. A new company could start with a modern data stack (e.g. Snowflake/BigQuery for a warehouse, dbt for transformations, etc.) to get these benefits out of the gate. Duolingo had to clean up some inconsistent analytics at scale; starting fresh means you can enforce consistency from day one (naming, event schemas). Another tip is to invest in an experiment analysis platform – Duolingo built their own (internally often called “MetaMetrics” or similar) to automatically crunch experiment results. New teams might use off-the-shelf solutions (like Statsig or homemade scripts) to ensure statistical rigor. One thing Duolingo could have done earlier is implement real-time analytics for faster experiment readouts – it’s not clear if they have real-time dashboards or rely on daily batch data. In 2025, streaming analytics (Kafka/Flink or cloud equivalents) are more accessible, so a new product might get insights in minutes rather than waiting a day. Overall, Duolingo’s data practices highlight the importance of building a data-informed culture: treat experiment results as the arbiter of truth (not the HiPPO), and build the tech to support that mentality.

5. Design Systems & Gamification

Duolingo’s product is often described as “game-like”, and this is very much by design. Over the years, they have developed a rich set of design systems and gamification features to keep learners motivated. Let’s explore how these have evolved: streaks, leaderboards, badges, widgets, and more.

The core gamification mechanic in Duolingo has always been the daily streak – the count of how many days in a row you’ve practiced. This simple concept drives huge engagement (many users are obsessed with not breaking their streak). Duolingo has doubled down on streaks in new ways. In 2022, they introduced the Friend Streak feature, where you can maintain a streak with friends as a shared goal. If you and I have a Friend Streak of 10, it means we both did our lesson each day for 10 days straight. This taps into accountability and social motivation. Early data showed having even one Friend Streak makes a learner 22% more likely to complete their lesson each day. Duolingo capitalized on this by allowing up to 5 friend streaks and even creating things like Friends Quest challenges (short team quests with a friend). For a new product, the takeaway is that social features can significantly boost retention – even if the core activity is single-player, knowing a friend is learning alongside you (even loosely) increases commitment. One clever aspect of Friend Streak is that it’s asynchronous social; you don’t need to be online together or actually interact beyond accepting the streak link, which lowered the barrier to entry (no scheduling, no real-time coordination). This was a design choice to keep it simple yet effective – akin to “studying next to a friend at the library” rather than directly collaborating. A modern app might consider similar lightweight social features (like shared goals or mutual progress tracking) to leverage social pressure positively without requiring complex multi-user interactions.

Another major evolution was Achievements and Badges. Duolingo had achievements (like “Sharpshooter – complete 100 lessons without a mistake”) buried in the profile for years, but they realized this system wasn’t reaching its potential ￼. In late 2023, they redesigned Achievements to make them more visible, celebratory, and shareable ￼. They introduced dozens of new badges covering different aspects of learning: some for major milestones (like a 365-day streak), some for personal bests (“Highest XP in a day”), and others for fun challenges (“Night Owl” for doing lessons late at night). They also added levels or tiers to certain badges, and crucially, made a new “Personal Bests” category to let users celebrate beating their own records. The redesign put all these achievements on the user’s profile in a sort of trophy cabinet that can be easily accessed and shown to friends. Embedded Image: On the left, you see the old profile Achievements view – only a few badges listed in text. On the right, the new Achievements section shows colorful badges for Personal Records and Awards, with Duolingo’s characters featured on badges. This revamp led to more users noticing and striving for badges. The lesson for product designers is that presentation and social sharing of achievements can amplify their effect. If achievements are hidden, they don’t motivate; but if they’re shiny and front-and-center, users will chase them. A new product should plan a way for users to feel proud of their milestones – whether via badges, titles, or other status symbols – and ideally let them share these with friends (Duolingo added share buttons for certain achievements so users post them on social media). It’s worth noting Duolingo ties achievements to actual learning progress (not random rewards), maintaining an alignment between gamification and educational value.

Duolingo’s Achievements Before vs After redesign ￼. In the old version (left), only a few text-heavy badges showed on the profile. The new version (right) showcases Personal Records (e.g. longest streak, most XP in a day) and a variety of Awards with vibrant icons of Duo & friends. By surfacing achievements more prominently and making them visually appealing, Duolingo increased their motivational impact.

To keep users engaged daily, Duolingo also introduced things like the daily goal widget for mobile and push notifications from the mascot. The Duolingo Widget (for iOS/Android home screens) came out after iOS 14 allowed widgets. This widget shows at-a-glance your current streak and whether you’ve met your goal today. Building it was tricky – they started it as a hackathon project, but issues of utility and installation made it a year-long development ￼. They determined the widget should be extremely simple (just streak info) to “motivate action” – if it’s red meaning you haven’t practiced today, the hope is you tap it and do a lesson. On Android, implementing the widget was harder due to device variations, but they launched it in March 2023, and on iOS they even added a Lock Screen variant for iOS16 ￼. The payoff: 50% of learners who installed the widget kept a streak 6+ months long – clearly, the constant visual reminder works. For new apps, think about how you can become part of the user’s routine and environment. Small surface area features like widgets or daily reminder notifications, when done thoughtfully, significantly improve retention. (The missed opportunity would be not doing it – Duolingo waited a couple years after iOS widgets were possible, but once they did, it proved valuable.)

Duolingo’s design system also includes a consistent visual language with its playful owl (Duo) and cast, a custom font, and a bright color palette. They maintain a style guide (design.duolingo.com) and ensure new features feel cohesive. Importantly, they use those characters to inject fun – for instance, celebratory animations when you reach a 7-day streak (the app shows “🔥 You’re on Fire!” with Duo dancing). In fact, they created a special Streak Society for streak milestones (weekly, monthly, yearly streak celebrations). These are essentially achievement unlocks but framed as joining an elite club, which users love. The key design insight is to celebrate the user’s progress frequently. Small wins (completed a lesson, finished a unit, hit a week streak) all trigger some positive feedback in the UI. Modern gamification also means constant personalization: Duolingo’s home screen path was redesigned to adapt to each learner’s pace, inserting review levels when needed, and interweaving old and new material following the principle of spaced repetition. (Spaced repetition is a memory technique where you review learned items at increasing intervals to solidify them. Duolingo’s new learning path in 2022 made spaced repetition the default – you don’t “finish” a skill and never see it again; instead the path brings back concepts periodically, which research shows is better for retention.) For a new learning app, building a system that adapts to user performance – giving easier content when they struggle, or throwing in a review of past material – can greatly enhance effectiveness. It’s a design choice at the intersection of pedagogy and UX.

Another notable element is Duolingo’s use of in-app currency (gems) and a Shop for power-ups (like “Streak Freeze” to protect your streak if you miss a day). These add a layer of game economy. Users earn gems through practice and can spend them on fun outfits for the mascot or on those power-ups. This is fairly common in free-to-play games; Duolingo adopted it to reinforce the habit loop (earn rewards by doing lessons, spend them to safeguard or customize). A cautionary tale: Duolingo used to have a more complex virtual currency called Lingots; they later simplified to Gems and focused the shop mostly on things that support learning (the Shop was refreshed as part of SDUI efforts to test layouts that highlight their subscription upsell and useful items). For new products, if you add a currency or point system, ensure it’s easily understood and actually drives the behavior you want (in Duolingo’s case, gems mainly serve to motivate streak keeping and upsell to their premium “Super Duolingo” which removes ads and gives extra perks).

Gamification trade-offs: It’s worth noting Duolingo has occasionally been critiqued for over-gamifying (e.g. some long-time users feel leaderboards or leagues make the app competitive in an unproductive way). Duolingo addressed this by continually refining these features – e.g. they added leagues (a weekly XP competition among 30 users) to spark engagement, but they had to tune the algorithm to prevent it from encouraging quantity of learning at the expense of quality. A new app should be mindful that gamification can backfire if it creates anxiety or if users game the system. Duolingo’s solution is to keep the focus on personal progress. The Achievements redesign explicitly added Personal Bests – competing with yourself, not others, which is a healthy form of motivation. The friend aspects are cooperative rather than adversarial. This is a smart direction for educational gamification in particular.

In summary, Duolingo’s design and gamification features have grown to form a full ecosystem of motivation mechanics: daily goals (streaks, XP targets), social reinforcement (friends, sharing achievements), visible progress (path, levels), surprise and delight (animations, quirky sentences), and tangible rewards (badges, gems). For an ambitious builder, the lesson is that engagement is driven by a mix of short-term hooks and long-term goals. Duolingo provides immediate feedback (points, animations each lesson) and long-term aspirations (finish the course, join the 365-day streak club, etc.). When creating a new product, consider how you will onboard users (Duolingo uses friendly characters and an intuitive tutorial), how you’ll keep them coming back tomorrow (maybe a streak or a reminder of their goal), and how you’ll keep them around for a year (bigger milestones, a learning “path” or journey). One missed opportunity Duolingo had historically was underestimating the social element – they only recently (2022-2024) really pushed features like Friend Streak and Friend Quests after realizing a majority of users had added friends. A new language app in 2025 might from the outset build more community features (study groups or user-generated content) to create network effects. However, one must integrate these in a way that doesn’t detract from learning – Duolingo’s characters and playful UI hit a nice balance where everything gamified still ties back to studying. A modern design system should strive for that harmony between fun and function.

6. Engineering Team Practices

Behind Duolingo’s product is a culture of continuous improvement, bold experimentation, and careful maintenance. The engineering team’s practices have evolved as the company grew from a scrappy startup (just one iOS developer in 2012) to a sizable tech org. Key practices include dogfooding, hackathons, thorough design docs, aggressive tech debt cleanup, and investing in developer productivity.

One hallmark is that Duolingo engineers and PMs dogfood new features extensively. As mentioned, the initial prototype of Friend Streak was built as a quick local prototype (all on-device, no backend) just to simulate the experience. The team played with it internally and found it fun, which helped convince leadership to green-light the real project. This idea of an “uber prototype” – even a hacked version – is great for de-risking innovative features. Duolingo also runs hackathons (they credit hackathons for spurring things like the DuoRadio scaling idea and the widget concept) ￼. Hackathons give engineers freedom to experiment outside the roadmap, often yielding creative solutions to big problems. For a new team, fostering a culture where engineers can try out wild ideas (and actually listening when those ideas show promise) can lead to major leaps forward. Duolingo’s leadership seems to encourage this and even uses prototypes to get buy-in for investing engineering effort in non-trivial projects.

When it comes to planning large technical projects, Duolingo learned the importance of consensus and documentation. In the async Python migration blog, the author notes that they first had to convince the org it was worth doing – which meant explaining the benefits in clear terms, doing a small proof-of-concept, and documenting how they’d approach it. Similarly, the Android architecture overhaul was preceded by a “tackle the monkey” exercise where a few engineers tried the new MVVM pattern on a single screen and then wrote thorough documentation and playbooks for migrating other parts of the app ￼. This documentation enabled ~30 developers to coordinate on the rewrite efficiently by following a guide ￼. The team even split into squads with designated leads to manage the effort in parallel ￼. This is textbook execution for a large refactor: prototype, document, mobilize the team with clear ownership. A startup in 2025 might not have 30 engineers, but the principle scales down – even a team of 3 needs to communicate designs and divide work when undertaking non-trivial changes. Duolingo’s CEO also banned the term “MVP” for new features, preferring to call it “V1” to set a mindset that even the first release should be a complete, quality product (if it ended up being the only version) ￼. This is more of a cultural note, but it shows Duolingo’s bias for polish. It’s a trade-off: sometimes MVPs are okay for learning, but for user-facing education features, Duolingo leans toward ensuring a solid baseline experience.

One area Duolingo really shines is technical debt management and performance monitoring. As the codebase expanded 20× over 10 years ￼, they recognized the risk of bloat and obsolete code. In 2024 they partnered with Emerge Tools to use a product called Reaper that finds unused code by analyzing what gets called at runtime (in beta test sessions) ￼ ￼. The integration was trivial (one line of code to include the SDK) ￼, and it gave them a dashboard listing classes and functions that were never invoked by any user ￼. The team was surprised at how much dead code lurked – old experiment variants, deprecated features that weren’t fully removed, etc. After double-checking that those classes weren’t needed, they blew away thousands of lines. In two passes, they deleted over 10,000 lines of unused code (about 1% of the iOS codebase) ￼. Embedded Image: Below is a screenshot of Reaper’s dashboard showing one run: it identified 2,684 unused classes out of 8,567 monitored in the Duolingo iOS app ￼ ￼. Having this data gave engineers confidence to remove whole chunks of code, resulting in a smaller, cleaner app. This also contributed to reducing app size and potentially improving performance (less code to load). A new project might not need such a tool immediately, but the lesson is to periodically audit your code for things that can be pruned. Don’t let forgotten code accumulate “just in case” – it can incur maintenance cost and even runtime overhead. Modern tools (like Reaper or static analyzers) can assist; even without them, you could schedule “spring cleaning” sprints to cull things that your metrics show aren’t being used.

Reaper dashboard from Duolingo’s iOS app, flagging unused classes. In this snapshot, 2,684 classes were unused out of ~8,567 monitored ￼. Such insights allowed Duolingo to safely delete large swaths of dead code, reducing app size and complexity by ~10k lines ￼.

Relatedly, Duolingo tackled app size bloat directly. By 2023 the iOS app had grown past 200 MB (download size), partly from accumulating years of assets and code ￼ ￼. They integrated Emerge Tools’ size analysis into every pull request to catch large additions ￼. This awareness led to decisions like converting old image assets to modern formats (SVGs where possible, removing unused illustrations) and dropping support for very old iOS versions to remove legacy code ￼. They achieved a ~20% size reduction with a focused effort ￼. The guidance for new apps is: keep an eye on your binary size, especially if targeting markets with limited connectivity. Use CI tools to monitor size regressions, and don’t be afraid to remove or compress assets. It’s easier to maintain a small app than to shrink a big app later.

On the DevOps and release process, Duolingo has a relatively rapid cycle: they do weekly app releases (mentioned in the app size blog) ￼ and likely even more frequent backend deployments (possibly daily). To enable this, they invested in continuous integration (fast builds, as discussed) and also built internal tools for release management. For instance, they replaced some Jenkins pipelines with custom scripts to remove bottlenecks and added automated checks. They also use feature flags extensively, which allows shipping code that’s off for users until it’s ready – this decouples deploy from launch. A new team should similarly adopt feature flags (there are services like LaunchDarkly, or simple config flags) to enable safe continuous deployment. Duolingo can push out a new version and not “turn on” a new feature until metrics from a test look good. This reduces risk and makes rollback simpler (just flip the flag off if needed).

In terms of team organization, Duolingo is known to keep teams small and mission-focused. They have “pods” or squads for different product areas (e.g. a pod for onboarding, a pod for the League feature, etc.). Engineers often rotate or collaborate across pods for big efforts (like the Android rewrite involved all Android engineers across feature teams working together for a period ￼). This flexibility to pause feature work and address core quality was crucial in 2021 to improve app ratings and stability. It shows a discipline to sometimes slow down and pay off tech debt, which not all companies manage to do. New teams should plan for periodic “stability months” or “quality sprints” where you focus on bug fixes, refactors, and performance, even if it means delaying new features. Duolingo’s improvement in Android performance post-reboot likely saved them many users and support tickets.

Security is another aspect: While Duolingo hasn’t had notable breaches (publicly at least), they do run a bug bounty program and have to protect a lot of student data. For the Duolingo English Test, they have stringent identity verification. A modern app should from the start use secure coding practices and possibly leverage cloud security tools (Duolingo likely uses AWS security features, encryption at rest, etc., though this isn’t detailed in blogs). One specific mention: Duolingo integrated the OpenAI API which brings privacy considerations – they likely ensure no sensitive user data is sent in prompts, etc., and that might be part of their security reviews.

Finally, one cannot overlook Duolingo’s focus on developer happiness and efficiency. The CI speedup, the automated testing with GPT, the code cleanup – these all aim to make engineering faster and more pleasant. Duolingo explicitly noted in their CI post that faster builds = happier developers = better product output. They also mentioned future plans like using AI to assist code reviews and merges. This forward-looking approach means the team is not just reacting to problems, but actively finding ways to work smarter (for example, the GPT Driver wasn’t a necessity – manual QA could have continued – but adopting it freed up QA and engineers to focus on more creative tasks). New teams should definitely automate repetitive tasks early – whether it’s test deployments, environment setup, or regression tests. With so many AI and DevOps tools available now, small teams can accomplish what used to take large teams.

If starting fresh today, an engineering team could take Duolingo’s lessons to heart by: setting up a strong experiment framework from day 1, keeping the codebase lean with continuous refactoring, using modern tooling for quality (linters, type checkers, CI pipelines, etc.), and building a culture where engineers feel ownership of both the user experience and the code health. One “missed” thing at Duolingo historically might be that they waited to address some tech debt (like app architecture) a bit late – they managed it, but it required halting feature work. A new team can try to incrementally refactor as they go (the challenge is balancing speed and quality – Duolingo sometimes chose speed then did a big fix later). Also, a new team could consider more use of open source frameworks – Duolingo built a lot in-house (experimentation system, data tooling). In 2025, you might use SaaS or OSS solutions to get similar capabilities faster, focusing your custom engineering on your unique product features.

To conclude, Duolingo’s engineering and product journey is a treasure trove of insights. They show how a product can evolve significantly (technically and UX-wise) while always tying changes back to core goals: making learning effective, accessible, and fun. By adopting a rigorous experimental approach, investing in both product features and the infrastructure to support them, and not shying away from bold changes (like rewrites or AI features), Duolingo has stayed at the cutting edge of edtech. An ambitious builder in 2025 can learn from Duolingo to combine educational science, gamification, and solid engineering to build the next big thing – and perhaps avoid some pitfalls by building on the patterns Duolingo has shown to work. As Duolingo’s story shows, success isn’t just one big innovation, but rather a continuous process of tweaking, testing, and innovating across the stack, from cloud infrastructure to the color of the “Congrats” screen. Each section of Duolingo’s evolution offers guidance on how to recreate or even surpass their system: use server-driven UIs for agility, choose scalable backend patterns early, harness AI thoughtfully for personalization and content, build a data-first culture, delight users with design and keep them motivated, and always empower your engineering team with the tools and time to keep the engine running smoothly. With these lessons, a new product team can stand on Duolingo’s shoulders and reach even greater heights in the edtech landscape.

Sources: Duolingo Engineering Blog ￼ ￼, and others as cited throughout.

Build-Out Roadmap – From Day-1 to “Duolingo-Level”

(Columns read left → right: where Duolingo is today, then three staged targets for “X Company”)

Stack / Capability	Duolingo Today	Phase 1 – Kick-off (Weeks 1-12)Ship fast, prove value	Phase 2 – Growth (Months 3-18)Harden & automate	Phase 3 – Scale (18 mo + )Rival Duolingo efficiency
Mobile Client	Native iOS / Android; Rive animations; server-driven UI for key surfaces	Expo + React Native (shared code) • OTA updates via Expo EAS Update	Migrate heavy screens to native modules where perf matters • Introduce SDUI JSON schema for Shop & Home	Split-repo “modular RN” or Kotlin Multiplatform/SwiftUI • Full SDUI coverage; dynamic assets streamed via CDN
Hosting / Compute	AWS ECS/K8s; Python & async micro-services; Go for infra glue	Begin on Vercel / Render (Node or FastAPI) • Monolith → easier ops	Dockerize > move to AWS Fargate • Break out auth & file-processing into micro-services	Clustered Kubernetes or ECS + service mesh • Autoscaling policies; blue-green deploys
Container & Virtualization	Docker everywhere; Infra as Code (Terraform)	Local Docker-Compose for dev; GitHub Actions build	ECR registry; IaC with Pulumi/Terraform; staging vs prod envs	Multi-region clusters; Chaos testing; Spot-instancer for cost
Databases	DynamoDB (31 B rows), RDS (MySQL/PG), Redis, S3	PostgreSQL (Supabase) for transactional + blob storage	Introduce Redis cache & S3 for media; partition Postgres for multi-tenant	Polyglot: Dynamo-like NoSQL for hot user-state, Aurora IO-Optimized for relational, tiered S3/Lake-house for analytics
Auth & User Mgmt	Home-grown plus social logins; rate-limiting & circuit-breakers	Clerk or Firebase Auth for speed	Custom auth micro-service; social + email; JWT, refresh tokens	Zero-trust stack; device fingerprinting; adaptive MFA
Content Management	Internal CMS + curriculum DB; SDL-based editing tools	Headless CMS (Sanity/Contentful) to hand-author lessons	Custom “Lesson Builder” app storing JSON in Postgres; role-based editor	WYSIWYG pipeline with versioning, crowdsourced review, and auto-gen TTS & visemes
AI / Content Gen	GPT-4 prompts, in-house TTS voices, viseme factory, Birdbrain adaptive model	⬩ Use OpenAI or Mistral APIs for “Explain answer” MVP ⬩ Off-the-shelf TTS (ElevenLabs)	Fine-tune LLM on domain data; Whisper + MFA for phoneme timings; cache inference via Redis	Train custom speech & dialog models; on-device quantized models; automated lip-sync pipeline à la Rive
Personalization / Spaced Rep	Birdbrain v3 – per-skill ELO, live A/B; writes to Dynamo	Simple Leitner queue stored per-user in Postgres	Incremental Bayesian model; nightly batch recompute on Snowflake	Real-time streaming feature store; multi-arm bandit & per-learner embeddings
Experimentation & Analytics	Hundreds of concurrent A/B tests; internal “Experiment Service”	Mixpanel/Amplitude + LaunchDarkly flags	Migrate events to PostHog or Segment → Snowflake; build metrics dashboards	In-house stats engine (statsig-like); auto-promote winners; cost telemetry wiring
CI / CD	Weekly app releases; <16 min mobile CI; Reaper dead-code scanning	GitHub Actions; Expo EAS build; manual TestFlight	Dedicated macOS & r7a runners; Gradle & Xcode caching; auto-semantic release	Multi-lane pipelines; canary deploys; AI-driven UI tests (GPT-Driver-style)
Caching & Edge	CloudFront CDN, device buckets, ETag tuning saved 6-figures	Rely on Vercel edge cache; HTTP-cache headers	Add CloudFront / BunnyCDN for media & JSON; versioned asset keys	Fine-grained TTL tuning; regional Dynamo DAX; edge compute for personalization
Observability & Cost	Datadog, CloudWatch, CloudZero for $/feature	Basic logs + Sentry + Vercel analytics	Grafana/Loki + OpenTelemetry traces; monthly cost reviews	Unified cost dashboards (CloudZero) tied to KPIs; anomaly alerts
Gamification & UX	Streaks, Friend Streak, quests, 100+ badges, widgets	Single daily streak + XP; push notif; simple achievements	Path UI with spaced review; home-screen widget; cooperative quests	Dynamic badges, leagues, seasonal events; AI-generated sentences & voice
Payments & Subscriptions	App Store / Play native IAP; Duolingo Max	RevenueCat wrapper for iOS/Android IAP	Tiered plans; promo codes; Stripe web paywall	Multi-SKU global pricing, tax handling, family plans
Reporting & BI	Looker + bespoke data diff & linters; blue-green datasets	Supabase dashboards / Metabase	dbt-modelled warehouse (Snowflake/BigQuery); data quality CI	Real-time stream analytics; ML-based anomaly detection
Voice & Speech Pipeline	Custom ASR, pronunciation scoring, viseme timings	Whisper API for ASR; store timestamps	Whisper + MFA alignment service; Rhubarb lip-sync for 2D avatars	Proprietary ASR; on-device forced alignment; auto-viseme factory

How to read it
	•	Phase 1 (Kick-off) – Lean SaaS choices; focus on fastest path to a lovable product.
	•	Phase 2 (Growth) – Modularize, introduce micro-services where wins are clear, start owning core IP (lesson builder, basic ML).
	•	Phase 3 (Scale) – Full cloud-native maturity: multi-region infra, in-house models, heavy automation, cost instrumentation everywhere.

Use this as a living blueprint: start in column 3, graduate row-by-row as traction (and cash) demand – and only adopt Phase 3 gear when the pain of NOT doing so exceeds the migration cost.


Audio hashing 

B. Personalized Learning at Scale

Beyond assessment, Duolingo heavily employs AI to personalize the learning experience and generate educational content efficiently across its diverse user base.

Duolingo Max & GPT-4: The introduction of Duolingo Max, a premium subscription tier, represents a significant step in leveraging large language models (LLMs) for enhanced learning features. This tier utilizes OpenAI's GPT-4 model to power two key functionalities:   

"Explain My Answer" provides learners with tailored explanations for why their answers in certain exercises were correct or incorrect, moving beyond simple right/wrong feedback to foster deeper understanding. Tapping a dedicated button triggers an AI-generated explanation contextualized to the specific exercise and the learner's response.   
"Roleplay" offers learners opportunities to practice real-world conversational skills by interacting with AI-powered characters in various scenarios (e.g., ordering coffee, discussing plans). These scenarios, presented as "Side Quests," are crafted by Duolingo's curriculum experts who also write the initial prompts and guide the AI on the conversation flow, ensuring relevance to the learner's progress. After the interaction, the AI provides feedback on the accuracy and complexity of the learner's responses, offering tips for improvement.   
User feedback mechanisms are integrated, allowing learners to rate explanations (thumbs up/down) and report inaccuracies in AI responses by long-pressing the message, contributing to model refinement.   
DuoRadio & Generative AI for Content: DuoRadio, an audio-based feature designed to improve listening comprehension, faced significant scaling challenges when content production relied on manual processes. Duolingo overcame this by employing generative AI to automate script creation.   

Content Pipeline: The automated pipeline starts by feeding the AI model with high-quality, language-specific sentences and exercises from Duolingo's existing course curriculum, rather than generic prompts. This curriculum-driven approach proved crucial for generating accurate and pedagogically sound content. Learner data was analyzed to standardize the placement of exercises within episodes, improving consistency. The AI generates numerous candidate scripts, which are then filtered by AI-powered evaluators using prompts crafted by Learning Designers. These evaluators assess scripts based on criteria such as naturalness, grammatical correctness, coherence, and logical flow, ensuring only high-quality content reaches learners. Approved scripts are automatically converted into audio using advanced Text-to-Speech (TTS) technology. This end-to-end automation enabled a dramatic increase in content, expanding from 300 episodes in 2 courses to over 15,000 episodes across 25+ courses, while reportedly saving 99% of production costs and boosting daily sessions tenfold (500K to 5M) in under six months.   
Audio Hashing: To ensure consistency for recurring elements like introductions and outros across thousands of episodes and multiple languages, Duolingo utilizes "advanced audio hashing". This technique likely involves generating a unique digital fingerprint (hash) for the standard intro/outro audio files. During automated episode assembly, the system can quickly retrieve and insert the correct, pre-generated audio segment by matching its hash, significantly reducing manual audio editing time. While various hashing techniques exist , audio fingerprinting methods  are specifically designed for identifying audio content, making them suitable for this purpose of ensuring consistent retrieval and insertion of standardized audio segments.   
Video Call with Lily & LLM Interaction Design: For speaking practice, Duolingo developed "Video Call with Lily," an interactive feature where learners converse with an AI character. This feature uses LLMs (mentioning ChatGPT, Claude, Gemini as examples) but employs sophisticated interaction design to ensure a controlled and effective learning experience.   

AI Architecture: Rather than free-form conversation, the interaction is governed by structured prompts defining three roles: the System (providing instructions written by Learning Designers on persona, CEFR level, assistance strategies), the Assistant (Lily, the AI bot, acting based on System instructions), and the User (the learner). Conversations follow a predictable four-part blueprint: Opener (scripted greeting), First Question (topic-setting, generated in a prep stage), Conversation (interactive back-and-forth guided by System prompts), and Closer (triggered after a set number of exchanges). Personalization is achieved through an AI memory mechanism ("List of Facts"); after each call, an LLM analyzes the transcript to extract key user details, which are fed back to the System for future calls. A separate "Conversation Prep" stage uses the LLM with specific criteria (CEFR level, target vocabulary) to generate an appropriate first question, preventing overload on the main conversational LLM. During the call, "Mid-call evaluations" allow the System to prompt Lily to adapt based on the learner's input, ensuring responsiveness and maintaining engagement (e.g., changing topic, rephrasing).   
Birdbrain & Personalization: Duolingo's internal system, "Birdbrain," is mentioned as the engine for personalizing practice sessions based on individual learner performance data. While details of the underlying models are sparse in these posts, its existence points to a long-standing effort in using ML for adaptive learning.   

